{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "material-richardson",
   "metadata": {},
   "source": [
    "# Graph classification with Graph Convolutional Networks in PyTorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "armed-republic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import torchnet as tnt\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trying-advice",
   "metadata": {},
   "source": [
    "## Load data and graph utils\n",
    "\n",
    "Here we load the MUTAG dataset as a `networkx` graph and transform it to a Pytorch dataset. Each node in the dataset contains a label from 0 to 6 which will be used as a one-hot-encoding feature vector. From the 188 graphs nodes, we will use 150 for training and the rest for validation. We have two classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "clear-polish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data are ready\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "cwd = os.getcwd()\n",
    "#add MUTAG data in the environment\n",
    "sys.path.append(cwd + '/../MUTAG')\n",
    "\n",
    "\n",
    "\"\"\" Download MUTAG dataset\"\"\"\n",
    "\"\"\" Extra graph utils and data loading stuff\"\"\"\n",
    "\n",
    "def indices_to_one_hot(number, nb_classes, label_dummy=-1):\n",
    "    \"\"\"Convert an iterable of indices to one-hot encoded labels.\n",
    "    \n",
    "    1. The expression np.eye(nb_classes)[number] is used to create a one-hot encoding representation of\n",
    "    a number with respect to the given number of classes (nb_classes). Let's break down the expression \n",
    "    step by step:\n",
    "\n",
    "    2. np.eye(nb_classes): This creates a 2D NumPy array (matrix) with shape (nb_classes, nb_classes)\n",
    "    where the diagonal elements are set to 1 and all other elements are set to 0. This is known as an identity matrix or a unit matrix.\n",
    "\n",
    "    3. [number]: This part of the expression is used to access a specific row in the matrix created in step 1.\n",
    "    Since the index is zero-based, number should be an integer between 0 and nb_classes - 1.\"\"\"\n",
    "    \n",
    "    if number == label_dummy:\n",
    "        return np.zeros(nb_classes)\n",
    "    else:\n",
    "        return np.eye(nb_classes)[number]\n",
    "\n",
    "def get_graph_signal(nx_graph):\n",
    "    d = dict((k, v) for k, v in nx_graph.nodes.items())\n",
    "    x = []\n",
    "    invd = {}\n",
    "    j = 0\n",
    "    for k, v in d.items():\n",
    "        x.append(v['attr_dict'])\n",
    "        invd[k] = j\n",
    "        j = j + 1\n",
    "    return np.array(x)\n",
    "\n",
    "\n",
    "def load_data(path, ds_name, use_node_labels=True, max_node_label=10):\n",
    "    node2graph = {}\n",
    "    Gs = [] #list of graphs\n",
    "    data = []\n",
    "    #Name and paths of each files are indicated\n",
    "    dataset_graph_indicator = f\"{ds_name}_graph_indicator.txt\" #which node in which 188 graphs\n",
    "    dataset_adj = f\"{ds_name}_A.txt\"\n",
    "    dataset_node_labels = f\"{ds_name}_node_labels.txt\"\n",
    "    dataset_graph_labels = f\"{ds_name}_graph_labels.txt\"\n",
    "\n",
    "    path_graph_indicator = os.path.join(path,dataset_graph_indicator)\n",
    "    path_adj = os.path.join(path,dataset_adj)\n",
    "    path_node_lab = os.path.join(path,dataset_node_labels)\n",
    "    path_labels = os.path.join(path,dataset_graph_labels)\n",
    "\n",
    "    #create graphs by nx and append it to list Gs\n",
    "    #from file graph indicator in which each node is labeled \n",
    "    #correponding to each graph\n",
    "    \n",
    "    with open(path_graph_indicator, \"r\") as f:\n",
    "        c = 1\n",
    "        for line in f:\n",
    "            node2graph[c] = int(line[:-1])#dict {node_index: graph_label}\n",
    "            if not node2graph[c] == len(Gs):\n",
    "                #if label of the node is not equal to the length\n",
    "                #of the graph list, this means that the current node belongs\n",
    "                #to the next graph, so a new graph network is created\n",
    "                #to appended to the Gs list, then the related nodes \n",
    "                #are added to the graph\n",
    "                Gs.append(nx.Graph())\n",
    "            Gs[-1].add_node(c)\n",
    "            c += 1\n",
    "\n",
    "    with open(path_adj, \"r\") as f:#Adjacency matrix\n",
    "        for line in f:\n",
    "            edge = line[:-1].split(\",\")\n",
    "            edge[1] = edge[1].replace(\" \", \"\")\n",
    "            #add edges from A matrix for each graph\n",
    "            Gs[node2graph[int(edge[0])] - 1].add_edge(int(edge[0]), int(edge[1]))\n",
    "\n",
    "            \n",
    "    #create one-hot encoding for the node label as the feature of each node\n",
    "    #the feature vector of each node is added to the graph\n",
    "    if use_node_labels:\n",
    "        with open(path_node_lab, \"r\") as f:\n",
    "            c = 1\n",
    "            for line in f:\n",
    "                node_label = indices_to_one_hot(int(line[:-1]), max_node_label)\n",
    "                Gs[node2graph[c] - 1].add_node(c, attr_dict=node_label)\n",
    "                c += 1\n",
    "\n",
    "    labels = []\n",
    "    with open(path_labels, \"r\") as f:\n",
    "        for line in f:\n",
    "            labels.append(int(line[:-1]))\n",
    "\n",
    "    return list(zip(Gs, labels)) \n",
    "\n",
    "def create_loaders(dataset, batch_size, split_id, offset=-1):\n",
    "    train_dataset = dataset[:split_id]\n",
    "    val_dataset = dataset[split_id:]\n",
    "    return to_pytorch_dataset(train_dataset, offset,batch_size), to_pytorch_dataset(val_dataset, offset,batch_size)\n",
    "\n",
    "def to_pytorch_dataset(dataset, label_offset=0, batch_size=1):\n",
    "    #graphs, labels = dataset\n",
    "    list_set = []\n",
    "    for graph, label in dataset:\n",
    "        #F:node feature vectors for each graph\n",
    "        #G: numpy array of graph\n",
    "        F, G = get_graph_signal(graph), nx.to_numpy_matrix(graph)\n",
    "        numOfNodes = G.shape[0]\n",
    "        F_tensor = torch.from_numpy(F).float()\n",
    "        G_tensor = torch.from_numpy(G).float()\n",
    "\n",
    "        # fix labels to zero-indexing\n",
    "        if label == -1:\n",
    "            label = 0\n",
    "\n",
    "        label += label_offset\n",
    "\n",
    "        list_set.append(tuple((F_tensor, G_tensor, label)))\n",
    "\n",
    "    dataset_tnt = tnt.dataset.ListDataset(list_set)\n",
    "    data_loader = torch.utils.data.DataLoader(dataset_tnt, shuffle=True, batch_size=batch_size)\n",
    "    return data_loader\n",
    "\n",
    "\n",
    "\n",
    "#dataset = list(zip(Gs, labels)) \n",
    "dataset = load_data(path='../MUTAG', ds_name='MUTAG',\n",
    "                  use_node_labels=True, max_node_label=7) \n",
    "\n",
    "# train_dataset = tuple((F_tensor(node feature vectors), G_tensor, label))\n",
    "# val_dataset = tuple((F_tensor(node feature vectors), G_tensor, label))\n",
    "\n",
    "train_dataset, val_dataset = create_loaders(dataset, batch_size=1, split_id=150, offset=0)\n",
    "print('Data are ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spiritual-ceramic",
   "metadata": {},
   "source": [
    "## GCN Layer\n",
    "\n",
    "GCNs are nothing more than a matrix multiplication between the input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "duplicate-carroll",
   "metadata": {},
   "outputs": [],
   "source": [
    "def device_as(x,y):\n",
    "    return x.to(y.device)\n",
    "\n",
    "# tensor operationa now support batched inputs\n",
    "def calc_degree_matrix_norm(a):\n",
    "    return torch.diag_embed(torch.pow(a.sum(dim=-1),-0.5)) #D^(-1/2)\n",
    "    \n",
    "    '''a.sum(dim=-1): This part of the expression calculates the sum of elements along the last dimension (-1)\n",
    "                      of the input tensor a. The result will be a tensor with one fewer dimension than the original\n",
    "                      a tensor.\n",
    "\n",
    "    torch.pow(a.sum(dim=-1), -0.5): This function applies an element-wise power operation to the tensor obtained \n",
    "                                    in step 1. It raises each element of the tensor to the power of -0.5.\n",
    "\n",
    "    torch.diag_embed(...): This function creates a diagonal matrix from the input tensor. \n",
    "                           It takes a tensor and returns a new tensor with the original tensor\n",
    "                           placed on the diagonal of a larger zero-filled matrix.'''\n",
    "    \n",
    "def create_graph_lapl_norm(a): #L_norm\n",
    "    size = a.shape[-1]\n",
    "    a +=  device_as(torch.eye(size),a) #A_norm = A + I\n",
    "    D_norm = calc_degree_matrix_norm(a) #D^(-1/2) --> diagonal\n",
    "    L_norm = torch.bmm( torch.bmm(D_norm, a) , D_norm )#L_norm = D^(-1/2)*A_norm*D^(-1/2)\n",
    "    return L_norm\n",
    "\n",
    "#1. BUILD YOUR GCN LAYER\n",
    "class GCN_Layer(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple GCN layer\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features, bias=bias) #X*W\n",
    "        \n",
    "\n",
    "    def forward(self, X, A):\n",
    "        \"\"\"\n",
    "        A: adjÎ±cency matrix\n",
    "        X: graph signal\n",
    "        \"\"\"\n",
    "        L = create_graph_lapl_norm(A) #D^(-1/2) (A + I) D^(-1/2)\n",
    "        x = self.linear(X)\n",
    "        return torch.bmm(L, x)\n",
    "       \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acceptable-heaven",
   "metadata": {},
   "source": [
    "## Graph Neural Network\n",
    "\n",
    "Now let's stack 3 `GCN_Layer` in order to construct a full Graph Neural Network. The GNN is followed by a `Linear` layer that will output the final classification between the 2 categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "otherwise-pearl",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class GNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                    in_features = 7,\n",
    "                    hidden_dim = 64,\n",
    "                    classes = 2,\n",
    "                    dropout = 0.5):\n",
    "        super(GNN, self).__init__()\n",
    "\n",
    "        self.conv1 = GCN_Layer(in_features, hidden_dim)\n",
    "        self.conv2 = GCN_Layer(hidden_dim, hidden_dim)\n",
    "        self.conv3 = GCN_Layer(hidden_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, classes)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x,A):\n",
    "        x = self.conv1(x, A)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, A)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x, A)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        # aggregate node embeddings\n",
    "        x = x.mean(dim=1)\n",
    "        # final classification layer\n",
    "        return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premium-hospital",
   "metadata": {},
   "source": [
    "## Training loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "competent-shark",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda\n",
      "Epoch: 010, Train Acc: 0.6600, Val Acc: 0.6842 || Best Val Score: 0.6842 (Epoch 001) \n",
      "Epoch: 020, Train Acc: 0.6600, Val Acc: 0.6842 || Best Val Score: 0.6842 (Epoch 001) \n",
      "Epoch: 030, Train Acc: 0.7067, Val Acc: 0.6579 || Best Val Score: 0.7632 (Epoch 027) \n",
      "Epoch: 040, Train Acc: 0.7400, Val Acc: 0.6842 || Best Val Score: 0.7632 (Epoch 027) \n",
      "Epoch: 050, Train Acc: 0.7200, Val Acc: 0.7105 || Best Val Score: 0.7632 (Epoch 027) \n",
      "Epoch: 060, Train Acc: 0.7400, Val Acc: 0.7105 || Best Val Score: 0.7632 (Epoch 027) \n",
      "Epoch: 070, Train Acc: 0.7333, Val Acc: 0.6579 || Best Val Score: 0.7632 (Epoch 027) \n",
      "Epoch: 080, Train Acc: 0.7467, Val Acc: 0.7105 || Best Val Score: 0.7632 (Epoch 027) \n",
      "Epoch: 090, Train Acc: 0.7667, Val Acc: 0.6842 || Best Val Score: 0.7632 (Epoch 027) \n",
      "Epoch: 100, Train Acc: 0.7467, Val Acc: 0.7105 || Best Val Score: 0.7632 (Epoch 027) \n",
      "Epoch: 110, Train Acc: 0.7467, Val Acc: 0.7368 || Best Val Score: 0.7632 (Epoch 027) \n",
      "Epoch: 120, Train Acc: 0.7400, Val Acc: 0.7368 || Best Val Score: 0.7632 (Epoch 027) \n",
      "Epoch: 130, Train Acc: 0.7467, Val Acc: 0.7632 || Best Val Score: 0.7632 (Epoch 027) \n",
      "Epoch: 140, Train Acc: 0.7533, Val Acc: 0.6579 || Best Val Score: 0.7632 (Epoch 027) \n",
      "Epoch: 150, Train Acc: 0.7867, Val Acc: 0.7632 || Best Val Score: 0.7632 (Epoch 027) \n",
      "Epoch: 160, Train Acc: 0.7467, Val Acc: 0.6579 || Best Val Score: 0.7895 (Epoch 159) \n",
      "Epoch: 170, Train Acc: 0.7867, Val Acc: 0.7632 || Best Val Score: 0.8158 (Epoch 166) \n",
      "Epoch: 180, Train Acc: 0.7467, Val Acc: 0.6842 || Best Val Score: 0.8158 (Epoch 166) \n",
      "Epoch: 190, Train Acc: 0.7867, Val Acc: 0.7105 || Best Val Score: 0.8158 (Epoch 166) \n",
      "Epoch: 200, Train Acc: 0.7467, Val Acc: 0.6579 || Best Val Score: 0.8158 (Epoch 166) \n",
      "Epoch: 210, Train Acc: 0.7867, Val Acc: 0.7632 || Best Val Score: 0.8158 (Epoch 166) \n",
      "Epoch: 220, Train Acc: 0.7467, Val Acc: 0.6316 || Best Val Score: 0.8158 (Epoch 166) \n",
      "Epoch: 230, Train Acc: 0.7867, Val Acc: 0.7632 || Best Val Score: 0.8158 (Epoch 166) \n",
      "Epoch: 240, Train Acc: 0.7467, Val Acc: 0.6316 || Best Val Score: 0.8158 (Epoch 166) \n"
     ]
    }
   ],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f'Training on {device}')\n",
    "model = GNN(in_features = 7,\n",
    "                hidden_dim = 128,\n",
    "                classes = 2).to(device)\n",
    "\n",
    "optimizer= torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "def train(train_loader):\n",
    "    model.train()\n",
    "\n",
    "    for data in train_loader: \n",
    "        optimizer.zero_grad()  \n",
    "        X, A, labels = data\n",
    "        X, A, labels = X.to(device), A.to(device), labels.to(device)  \n",
    "        # Forward pass.\n",
    "        out = model(X, A)  \n",
    "        # Compute the graph classification loss.\n",
    "        loss = criterion(out, labels) \n",
    "        # Calculate gradients.\n",
    "        loss.backward()  \n",
    "        # Updates the models parameters\n",
    "        optimizer.step() \n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        X,A, labels = data\n",
    "        X, A, labels = X.to(device), A.to(device), labels.to(device) \n",
    "        # Forward pass.\n",
    "        out = model(X, A)  \n",
    "        # Take the index of the class with the highest probability.\n",
    "        pred = out.argmax(dim=1) \n",
    "        # Compare with ground-truth labels.\n",
    "        correct += int((pred == labels).sum()) \n",
    "    return correct / len(loader.dataset)  \n",
    "\n",
    "best_val = -1\n",
    "for epoch in range(1, 241):\n",
    "    train(train_dataset)\n",
    "    train_acc = test(train_dataset)\n",
    "    val_acc = test(val_dataset)\n",
    "    if val_acc>best_val:\n",
    "        best_val = val_acc\n",
    "        epoch_best = epoch\n",
    "    \n",
    "    if epoch%10==0:\n",
    "        print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f} || Best Val Score: {best_val:.4f} (Epoch {epoch_best:03d}) ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb863c82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

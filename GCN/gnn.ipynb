{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "material-richardson",
   "metadata": {},
   "source": [
    "# Graph classification with Graph Convolutional Networks in PyTorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "armed-republic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import torchnet as tnt\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trying-advice",
   "metadata": {},
   "source": [
    "## Load data and graph utils\n",
    "\n",
    "Here we load the MUTAG dataset as a `networkx` graph and transform it to a Pytorch dataset. Each node in the dataset contains a label from 0 to 6 which will be used as a one-hot-encoding feature vector. From the 188 graphs nodes, we will use 150 for training and the rest for validation. We have two classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "clear-polish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data are ready\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "cwd = os.getcwd()\n",
    "#add MUTAG data in the environment\n",
    "sys.path.append(cwd + '/../MUTAG')\n",
    "\n",
    "\n",
    "\"\"\" Download MUTAG dataset\"\"\"\n",
    "\"\"\" Extra graph utils and data loading stuff\"\"\"\n",
    "\n",
    "def indices_to_one_hot(number, nb_classes, label_dummy=-1):\n",
    "    \"\"\"Convert an iterable of indices to one-hot encoded labels.\n",
    "    \n",
    "    1. The expression np.eye(nb_classes)[number] is used to create a one-hot encoding representation of\n",
    "    a number with respect to the given number of classes (nb_classes). Let's break down the expression \n",
    "    step by step:\n",
    "\n",
    "    2. np.eye(nb_classes): This creates a 2D NumPy array (matrix) with shape (nb_classes, nb_classes)\n",
    "    where the diagonal elements are set to 1 and all other elements are set to 0. This is known as an identity matrix or a unit matrix.\n",
    "\n",
    "    3. [number]: This part of the expression is used to access a specific row in the matrix created in step 1.\n",
    "    Since the index is zero-based, number should be an integer between 0 and nb_classes - 1.\"\"\"\n",
    "    \n",
    "    if number == label_dummy:\n",
    "        return np.zeros(nb_classes)\n",
    "    else:\n",
    "        return np.eye(nb_classes)[number]\n",
    "\n",
    "def get_graph_signal(nx_graph):\n",
    "    d = dict((k, v) for k, v in nx_graph.nodes.items())\n",
    "    x = []\n",
    "    invd = {}\n",
    "    j = 0\n",
    "    for k, v in d.items():\n",
    "        x.append(v['attr_dict'])\n",
    "        invd[k] = j\n",
    "        j = j + 1\n",
    "    return np.array(x)\n",
    "\n",
    "\n",
    "def load_data(path, ds_name, use_node_labels=True, max_node_label=10):\n",
    "    node2graph = {}\n",
    "    Gs = [] #list of graphs\n",
    "    data = []\n",
    "    #Name and paths of each files are indicated\n",
    "    dataset_graph_indicator = f\"{ds_name}_graph_indicator.txt\" #which node in which 188 graphs\n",
    "    dataset_adj = f\"{ds_name}_A.txt\"\n",
    "    dataset_node_labels = f\"{ds_name}_node_labels.txt\"\n",
    "    dataset_graph_labels = f\"{ds_name}_graph_labels.txt\"\n",
    "\n",
    "    path_graph_indicator = os.path.join(path,dataset_graph_indicator)\n",
    "    path_adj = os.path.join(path,dataset_adj)\n",
    "    path_node_lab = os.path.join(path,dataset_node_labels)\n",
    "    path_labels = os.path.join(path,dataset_graph_labels)\n",
    "\n",
    "    #create graphs by nx and append it to list Gs\n",
    "    #from file graph indicator in which each node is labeled \n",
    "    #correponding to each graph\n",
    "    \n",
    "    with open(path_graph_indicator, \"r\") as f:\n",
    "        c = 1\n",
    "        for line in f:\n",
    "            node2graph[c] = int(line[:-1])#dict {node_index: graph_label}\n",
    "            if not node2graph[c] == len(Gs):\n",
    "                #if label of the node is not equal to the length\n",
    "                #of the graph list, this means that the current node belongs\n",
    "                #to the next graph, so a new graph network is created\n",
    "                #to appended to the Gs list, then the related nodes \n",
    "                #are added to the graph\n",
    "                Gs.append(nx.Graph())\n",
    "            Gs[-1].add_node(c)\n",
    "            c += 1\n",
    "\n",
    "    with open(path_adj, \"r\") as f:#Adjacency matrix\n",
    "        for line in f:\n",
    "            edge = line[:-1].split(\",\")\n",
    "            edge[1] = edge[1].replace(\" \", \"\")\n",
    "            #add edges from A matrix for each graph\n",
    "            Gs[node2graph[int(edge[0])] - 1].add_edge(int(edge[0]), int(edge[1]))\n",
    "\n",
    "            \n",
    "    #create one-hot encoding for the node label as the feature of each node\n",
    "    #the feature vector of each node is added to the graph\n",
    "    if use_node_labels:\n",
    "        with open(path_node_lab, \"r\") as f:\n",
    "            c = 1\n",
    "            for line in f:\n",
    "                node_label = indices_to_one_hot(int(line[:-1]), max_node_label)\n",
    "                Gs[node2graph[c] - 1].add_node(c, attr_dict=node_label)\n",
    "                c += 1\n",
    "\n",
    "    labels = []\n",
    "    with open(path_labels, \"r\") as f:\n",
    "        for line in f:\n",
    "            labels.append(int(line[:-1]))\n",
    "\n",
    "    return list(zip(Gs, labels)) \n",
    "\n",
    "def create_loaders(dataset, batch_size, split_id, offset=-1):\n",
    "    train_dataset = dataset[:split_id]\n",
    "    val_dataset = dataset[split_id:]\n",
    "    return to_pytorch_dataset(train_dataset, offset,batch_size), to_pytorch_dataset(val_dataset, offset,batch_size)\n",
    "\n",
    "def to_pytorch_dataset(dataset, label_offset=0, batch_size=1):\n",
    "    #graphs, labels = dataset\n",
    "    list_set = []\n",
    "    for graph, label in dataset:\n",
    "        #F:node feature vectors for each graph\n",
    "        #G: numpy array of graph\n",
    "        F, G = get_graph_signal(graph), nx.to_numpy_matrix(graph)\n",
    "        numOfNodes = G.shape[0]\n",
    "        F_tensor = torch.from_numpy(F).float()\n",
    "        G_tensor = torch.from_numpy(G).float()\n",
    "\n",
    "        # fix labels to zero-indexing\n",
    "        if label == -1:\n",
    "            label = 0\n",
    "\n",
    "        label += label_offset\n",
    "\n",
    "        list_set.append(tuple((F_tensor, G_tensor, label)))\n",
    "\n",
    "    dataset_tnt = tnt.dataset.ListDataset(list_set)\n",
    "    data_loader = torch.utils.data.DataLoader(dataset_tnt, shuffle=True, batch_size=batch_size)\n",
    "    return data_loader\n",
    "\n",
    "\n",
    "\n",
    "#dataset = list(zip(Gs, labels)) \n",
    "dataset = load_data(path='../MUTAG', ds_name='MUTAG',\n",
    "                  use_node_labels=True, max_node_label=7) \n",
    "\n",
    "# train_dataset = tuple((F_tensor(node feature vectors), G_tensor, label))\n",
    "# val_dataset = tuple((F_tensor(node feature vectors), G_tensor, label))\n",
    "\n",
    "train_dataset, val_dataset = create_loaders(dataset, batch_size=1, split_id=150, offset=0)\n",
    "print('Data are ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spiritual-ceramic",
   "metadata": {},
   "source": [
    "## GCN Layer\n",
    "\n",
    "GCNs are nothing more than a matrix multiplication between the input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "duplicate-carroll",
   "metadata": {},
   "outputs": [],
   "source": [
    "def device_as(x,y):\n",
    "    return x.to(y.device)\n",
    "\n",
    "# tensor operationa now support batched inputs\n",
    "def calc_degree_matrix_norm(a):\n",
    "    return torch.diag_embed(torch.pow(a.sum(dim=-1),-0.5)) #D^(-1/2)\n",
    "    \n",
    "    '''a.sum(dim=-1): This part of the expression calculates the sum of elements along the last dimension (-1)\n",
    "                      of the input tensor a. The result will be a tensor with one fewer dimension than the original\n",
    "                      a tensor.\n",
    "\n",
    "    torch.pow(a.sum(dim=-1), -0.5): This function applies an element-wise power operation to the tensor obtained \n",
    "                                    in step 1. It raises each element of the tensor to the power of -0.5.\n",
    "\n",
    "    torch.diag_embed(...): This function creates a diagonal matrix from the input tensor. \n",
    "                           It takes a tensor and returns a new tensor with the original tensor\n",
    "                           placed on the diagonal of a larger zero-filled matrix.'''\n",
    "    \n",
    "def create_graph_lapl_norm(a): #L_norm\n",
    "    size = a.shape[-1]\n",
    "    a +=  device_as(torch.eye(size),a) #A_norm = A + I\n",
    "    D_norm = calc_degree_matrix_norm(a) #D^(-1/2) --> diagonal\n",
    "    L_norm = torch.bmm( torch.bmm(D_norm, a) , D_norm )#L_norm = D^(-1/2)*A_norm*D^(-1/2)\n",
    "    return L_norm\n",
    "\n",
    "#1. BUILD YOUR GCN LAYER\n",
    "class GCN_Layer(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple GCN layer\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features, bias=bias) #X*W\n",
    "        \n",
    "\n",
    "    def forward(self, X, A):\n",
    "        \"\"\"\n",
    "        A: adjÎ±cency matrix\n",
    "        X: graph signal\n",
    "        \"\"\"\n",
    "        L = create_graph_lapl_norm(A) #D^(-1/2) (A + I) D^(-1/2)\n",
    "        x = self.linear(X)\n",
    "        return torch.bmm(L, x)\n",
    "       \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acceptable-heaven",
   "metadata": {},
   "source": [
    "## Graph Neural Network\n",
    "\n",
    "Now let's stack 3 `GCN_Layer` in order to construct a full Graph Neural Network. The GNN is followed by a `Linear` layer that will output the final classification between the 2 categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "otherwise-pearl",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class GNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                    in_features = 7,\n",
    "                    hidden_dim = 64,\n",
    "                    classes = 2,\n",
    "                    dropout = 0.5):\n",
    "        super(GNN, self).__init__()\n",
    "\n",
    "        self.conv1 = GCN_Layer(in_features, hidden_dim)\n",
    "        self.conv2 = GCN_Layer(hidden_dim, hidden_dim)\n",
    "        self.conv3 = GCN_Layer(hidden_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, classes)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x,A):\n",
    "        x = self.conv1(x, A)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, A)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x, A)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        # aggregate node embeddings\n",
    "        x = x.mean(dim=1)\n",
    "        # final classification layer\n",
    "        return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premium-hospital",
   "metadata": {},
   "source": [
    "## Training loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "competent-shark",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1249897/3453508871.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m241\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mval_acc\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0mbest_val\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1249897/3453508871.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(loader)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# Forward pass.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0;31m# Take the index of the class with the highest probability.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1249897/1657936610.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, A)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1249897/1056186892.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X, A)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \"\"\"\n\u001b[1;32m     42\u001b[0m         \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph_lapl_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#D^(-1/2) (A + I) D^(-1/2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)"
     ]
    }
   ],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f'Training on {device}')\n",
    "model = GNN(in_features = 7,\n",
    "                hidden_dim = 128,\n",
    "                classes = 2).to(device)\n",
    "\n",
    "optimizer= torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "def train(train_loader):\n",
    "    model.train()\n",
    "\n",
    "    for data in train_loader: \n",
    "        optimizer.zero_grad()  \n",
    "        X, A, labels = data\n",
    "        X, A, labels = X.to(device), A.to(device), labels.to(device)  \n",
    "        # Forward pass.\n",
    "        out = model(X, A)  \n",
    "        # Compute the graph classification loss.\n",
    "        loss = criterion(out, labels) \n",
    "        # Calculate gradients.\n",
    "        loss.backward()  \n",
    "        # Updates the models parameters\n",
    "        optimizer.step() \n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        X,A, labels = data\n",
    "        X, A, labels = X.to(device), A.to(device), labels.to(device) \n",
    "        # Forward pass.\n",
    "        out = model(X, A)  \n",
    "        # Take the index of the class with the highest probability.\n",
    "        pred = out.argmax(dim=1) \n",
    "        # Compare with ground-truth labels.\n",
    "        correct += int((pred == labels).sum()) \n",
    "    return correct / len(loader.dataset)  \n",
    "\n",
    "best_val = -1\n",
    "for epoch in range(1, 241):\n",
    "    train(train_dataset)\n",
    "    train_acc = test(train_dataset)\n",
    "    val_acc = test(val_dataset)\n",
    "    if val_acc>best_val:\n",
    "        best_val = val_acc\n",
    "        epoch_best = epoch\n",
    "    \n",
    "    if epoch%10==0:\n",
    "        print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f} || Best Val Score: {best_val:.4f} (Epoch {epoch_best:03d}) ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05abec91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1f2b0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0580607",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e879f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
